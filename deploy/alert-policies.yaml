# Google Cloud Monitoring 알람 정책 설정
# 중요한 메트릭에 대한 자동 알림 구성

# =================================
# 알람 정책 목록
# =================================

alertPolicies:
  # =================================
  # 1. 높은 에러율 알람
  # =================================
  - displayName: "[CRITICAL] Writerly 높은 에러율"
    documentation:
      content: |
        Cloud Run 서비스에서 5분간 에러율이 5%를 초과했습니다.
        
        **확인 사항:**
        1. Cloud Run 로그 확인
        2. Vertex AI 서비스 상태 확인
        3. Redis 연결 상태 확인
        4. 최근 배포 여부 확인
        
        **대응 방법:**
        - 즉시 롤백 고려
        - 에러 로그 분석
        - 팀 Slack 채널에 상황 공유
      mimeType: "text/markdown"
    
    conditions:
      - displayName: "높은 에러율 감지"
        conditionThreshold:
          filter: >
            resource.type="cloud_run_revision" AND
            resource.labels.service_name="writerly-slack-ai" AND
            metric.type="run.googleapis.com/request_count" AND
            metric.labels.response_code_class!="2xx"
          
          aggregations:
            - alignmentPeriod: "300s"
              perSeriesAligner: "ALIGN_RATE"
              crossSeriesReducer: "REDUCE_SUM"
          
          comparison: "COMPARISON_GT"
          thresholdValue: 0.05  # 5%
          duration: "300s"
          
          trigger:
            count: 1
    
    alertStrategy:
      autoClose: "1800s"  # 30분 후 자동 종료
    
    enabled: true
    
    notificationChannels:
      - "${NOTIFICATION_CHANNEL_EMAIL}"
      - "${NOTIFICATION_CHANNEL_SLACK}"

  # =================================
  # 2. 응답 시간 지연 알람
  # =================================
  - displayName: "[WARNING] Writerly 응답 시간 지연"
    documentation:
      content: |
        Cloud Run 서비스의 95% 응답 시간이 5초를 초과했습니다.
        
        **확인 사항:**
        1. CPU/메모리 사용률 확인
        2. Vertex AI 응답 시간 확인
        3. Redis 성능 확인
        4. 동시 요청 수 확인
        
        **대응 방법:**
        - 인스턴스 스케일링 고려
        - 코드 성능 최적화 검토
        - 캐싱 전략 점검
      mimeType: "text/markdown"
    
    conditions:
      - displayName: "응답 시간 지연 감지"
        conditionThreshold:
          filter: >
            resource.type="cloud_run_revision" AND
            resource.labels.service_name="writerly-slack-ai" AND
            metric.type="run.googleapis.com/request_latencies"
          
          aggregations:
            - alignmentPeriod: "300s"
              perSeriesAligner: "ALIGN_DELTA"
              crossSeriesReducer: "REDUCE_PERCENTILE_95"
          
          comparison: "COMPARISON_GT"
          thresholdValue: 5000  # 5초 (밀리초)
          duration: "600s"  # 10분
          
          trigger:
            count: 1
    
    enabled: true
    
    notificationChannels:
      - "${NOTIFICATION_CHANNEL_EMAIL}"

  # =================================
  # 3. 메모리 사용률 높음 알람
  # =================================
  - displayName: "[WARNING] Writerly 높은 메모리 사용률"
    documentation:
      content: |
        Cloud Run 컨테이너의 메모리 사용률이 85%를 초과했습니다.
        
        **확인 사항:**
        1. 메모리 누수 여부 확인
        2. 큰 요청 처리 중인지 확인
        3. 가비지 컬렉션 로그 확인
        
        **대응 방법:**
        - 메모리 할당량 증가 고려
        - 코드 메모리 사용 최적화
        - 큰 객체 처리 방식 개선
      mimeType: "text/markdown"
    
    conditions:
      - displayName: "높은 메모리 사용률 감지"
        conditionThreshold:
          filter: >
            resource.type="cloud_run_revision" AND
            resource.labels.service_name="writerly-slack-ai" AND
            metric.type="run.googleapis.com/container/memory/utilizations"
          
          aggregations:
            - alignmentPeriod: "300s"
              perSeriesAligner: "ALIGN_MEAN"
              crossSeriesReducer: "REDUCE_MEAN"
          
          comparison: "COMPARISON_GT"
          thresholdValue: 0.85  # 85%
          duration: "600s"  # 10분
          
          trigger:
            count: 1
    
    enabled: true
    
    notificationChannels:
      - "${NOTIFICATION_CHANNEL_EMAIL}"

  # =================================
  # 4. Redis 연결 문제 알람
  # =================================
  - displayName: "[CRITICAL] Redis 연결 문제"
    documentation:
      content: |
        Redis 인스턴스 연결에 문제가 발생했습니다.
        
        **확인 사항:**
        1. Redis 인스턴스 상태 확인
        2. VPC 네트워킹 상태 확인
        3. 인증 정보 확인
        
        **대응 방법:**
        - Redis 인스턴스 재시작 고려
        - 네트워크 설정 점검
        - 백업에서 복구 준비
      mimeType: "text/markdown"
    
    conditions:
      - displayName: "Redis 연결 수 급감"
        conditionThreshold:
          filter: >
            resource.type="redis_instance" AND
            resource.labels.instance_id="writerly-redis" AND
            metric.type="redis.googleapis.com/clients/connected"
          
          aggregations:
            - alignmentPeriod: "60s"
              perSeriesAligner: "ALIGN_MEAN"
          
          comparison: "COMPARISON_LT"
          thresholdValue: 1  # 연결 수 1개 미만
          duration: "300s"  # 5분
          
          trigger:
            count: 1
    
    enabled: true
    
    notificationChannels:
      - "${NOTIFICATION_CHANNEL_EMAIL}"
      - "${NOTIFICATION_CHANNEL_SLACK}"

  # =================================
  # 5. AI 응답 실패율 높음 알람
  # =================================
  - displayName: "[WARNING] AI 응답 실패율 높음"
    documentation:
      content: |
        Vertex AI 응답 실패율이 10%를 초과했습니다.
        
        **확인 사항:**
        1. Vertex AI 서비스 상태 확인
        2. API 할당량 확인
        3. 네트워크 연결 상태 확인
        
        **대응 방법:**
        - API 키/인증 확인
        - 재시도 로직 점검
        - 대체 AI 모델 고려
      mimeType: "text/markdown"
    
    conditions:
      - displayName: "AI 응답 실패율 높음"
        conditionThreshold:
          filter: >
            resource.type="global" AND
            metric.type="custom.googleapis.com/writerly/ai_requests_total" AND
            metric.labels.status="error"
          
          aggregations:
            - alignmentPeriod: "300s"
              perSeriesAligner: "ALIGN_RATE"
              crossSeriesReducer: "REDUCE_SUM"
          
          comparison: "COMPARISON_GT"
          thresholdValue: 0.1  # 10%
          duration: "600s"  # 10분
          
          trigger:
            count: 1
    
    enabled: true
    
    notificationChannels:
      - "${NOTIFICATION_CHANNEL_EMAIL}"

  # =================================
  # 6. 비정상적인 토큰 사용량 알람
  # =================================
  - displayName: "[INFO] 비정상적인 토큰 사용량"
    documentation:
      content: |
        시간당 토큰 사용량이 평소보다 300% 이상 증가했습니다.
        
        **확인 사항:**
        1. 대량 요청 발생 여부
        2. 스팸/악용 요청 확인
        3. 입력 길이 제한 동작 확인
        
        **대응 방법:**
        - 사용량 모니터링 강화
        - Rate limiting 점검
        - 비용 알림 확인
      mimeType: "text/markdown"
    
    conditions:
      - displayName: "토큰 사용량 급증"
        conditionThreshold:
          filter: >
            resource.type="global" AND
            metric.type="custom.googleapis.com/writerly/tokens_used"
          
          aggregations:
            - alignmentPeriod: "3600s"  # 1시간
              perSeriesAligner: "ALIGN_RATE"
              crossSeriesReducer: "REDUCE_SUM"
          
          comparison: "COMPARISON_GT"
          thresholdValue: 1000000  # 1M 토큰/시간
          duration: "300s"
          
          trigger:
            count: 1
    
    enabled: true
    
    notificationChannels:
      - "${NOTIFICATION_CHANNEL_EMAIL}"

  # =================================
  # 7. 서비스 다운 알람
  # =================================
  - displayName: "[CRITICAL] Writerly 서비스 다운"
    documentation:
      content: |
        Cloud Run 서비스가 응답하지 않습니다.
        
        **확인 사항:**
        1. 서비스 배포 상태 확인
        2. 헬스체크 로그 확인
        3. 컨테이너 시작 로그 확인
        
        **대응 방법:**
        - 즉시 이전 버전으로 롤백
        - 인시던트 대응 절차 시작
        - 팀 긴급 연락
      mimeType: "text/markdown"
    
    conditions:
      - displayName: "서비스 응답 없음"
        conditionAbsent:
          filter: >
            resource.type="cloud_run_revision" AND
            resource.labels.service_name="writerly-slack-ai" AND
            metric.type="run.googleapis.com/request_count"
          
          aggregations:
            - alignmentPeriod: "300s"
              perSeriesAligner: "ALIGN_RATE"
              crossSeriesReducer: "REDUCE_SUM"
          
          duration: "900s"  # 15분
          
          trigger:
            count: 1
    
    enabled: true
    
    notificationChannels:
      - "${NOTIFICATION_CHANNEL_EMAIL}"
      - "${NOTIFICATION_CHANNEL_SLACK}"
      - "${NOTIFICATION_CHANNEL_SMS}"

# =================================
# 알림 채널 설정 (환경 변수로 관리)
# =================================
notificationChannels:
  - name: "email-alerts"
    type: "email"
    displayName: "이메일 알림"
    description: "개발팀 이메일 알림"
    labels:
      email_address: "${ALERT_EMAIL}"
    enabled: true
    
  - name: "slack-alerts"
    type: "slack"
    displayName: "슬랙 알림"
    description: "개발팀 슬랙 채널 알림"
    labels:
      channel_name: "${SLACK_ALERT_CHANNEL}"
      url: "${SLACK_WEBHOOK_URL}"
    enabled: true
    
  - name: "sms-critical"
    type: "sms"
    displayName: "SMS 긴급 알림"
    description: "심각한 장애 시 SMS 알림"
    labels:
      number: "${EMERGENCY_PHONE}"
    enabled: true